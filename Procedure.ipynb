{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('Data/train.csv')\n",
    "trainLabels = pd.read_csv('Data/train_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[trainData < 0] = 0\n",
    "#Réduit de 73 features\n",
    "trainData = trainData.loc[:, (trainData != 0).any(axis=0)]\n",
    "#Réduit de 237 features\n",
    "trainData = trainData.loc[:, (trainData > 0.95).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecs import ignore_errors\n",
    "\n",
    "\n",
    "class LogisiticRegression:\n",
    "    def __init__(self, trainData, trainLabels):\n",
    "        self.trainData = trainData\n",
    "        self.trainLabels = trainLabels\n",
    "        self.featureSize = len(trainData.columns)\n",
    "        self.trainSize = len(self.trainData.index)\n",
    "        self.labels = np.sort(trainLabels['Class'].unique())\n",
    "        self.labelSize = self.labels.size\n",
    "\n",
    "        self.weights = pd.DataFrame(np.zeros((self.featureSize,self.labelSize)))\n",
    "        self.bias = pd.Series(np.zeros(self.labelSize))\n",
    "\n",
    "    def sigmoid(self,df):\n",
    "        result = np.zeros(len(df))\n",
    "\n",
    "        for i,val in enumerate(df):\n",
    "            result[i] = np.exp(val)/np.sum(np.exp(df))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def oneHotEncoder(self,Y):\n",
    "        #Transformer les labels en one hot pour calculer le loss\n",
    "        df = pd.DataFrame(np.zeros((self.labelSize,Y['Class'].size)))\n",
    "        for i,val in enumerate(Y['Class']):\n",
    "            ind = self.labels[self.labels==val]\n",
    "            df.loc[val,i] = 1\n",
    "        return df\n",
    "    \n",
    "    def predictSoftmax(self, df):\n",
    "        #Calculer le le produit de W^T*X^T+b pour tous les echantillons\n",
    "        newW = self.weights.transpose()\n",
    "        newX = df.transpose()\n",
    "        wx = newW.dot(newX.to_numpy())\n",
    "        gFunction = wx.transpose()+self.bias\n",
    "\n",
    "        #Faire le argmax(softmax(g(x))) pour tous les echantillons\n",
    "        softMaxResult = gFunction.apply(self.sigmoid,axis=0)\n",
    "\n",
    "        return softMaxResult\n",
    "\n",
    "    def predict(self, df):\n",
    "        #Calculer le le produit de W^T*X^T+b pour tous les echantillons\n",
    "        newW = self.weights.transpose()\n",
    "        newX = df.transpose()\n",
    "        wx = newW.dot(newX.to_numpy())\n",
    "        gFunction = wx.transpose()+self.bias\n",
    "\n",
    "        #Faire le argmax(softmax(g(x))) pour tous les echantillons\n",
    "        softMaxResult = gFunction.apply(self.sigmoid,axis=0)\n",
    "        predictions = softMaxResult.idxmax(axis=1)\n",
    "        predictionLabels = [self.labels[i] for i in predictions]\n",
    "\n",
    "        return predictionLabels\n",
    "    \n",
    "    def loss(self,Y,Yhat,batchSize):\n",
    "        #retourner la cout (entropie croise) \n",
    "        return (-np.sum(np.log(np.sum(Y.mul(Yhat.transpose()),axis=0))))/batchSize\n",
    "\n",
    "    def gradient(self,X,Y,Yhat,lam,bathSize):\n",
    "        #retourner le gradient avec une regularisation L2 \n",
    "        #Ajouter le biais au X et W pour avoir son gradient\n",
    "        X = pd.concat(X.transpose(),pd.Series(1,index=X.index),ignore_index=True)\n",
    "        W = pd.concat(self.weights,pd.Series(1,index=range(self.labelSize)),ignore_index=True)\n",
    "        return (X.dot((Y-Yhat.transpose()).transpose().to_numpy()))/bathSize + 2*lam*W.to_numpy()\n",
    "\n",
    "    def train(self, iter, batchSize, lam, alpha):\n",
    "        loss = pd.Series(np.zeros(iter))\n",
    "\n",
    "        for i in range(iter):\n",
    "            #Creer aleatoire la batch \n",
    "            sampleTrain = trainData.sample(batchSize,replace=False,axis=0)\n",
    "            sampleLabels = trainLabels.iloc[sampleTrain.index,:]\n",
    "            yOneHot = self.oneHotEncoder(sampleLabels)\n",
    "            \n",
    "            #Predire les probabilite des labels et calculer le cout\n",
    "            predictionProb = self.predictSoftmax(sampleTrain)\n",
    "            loss[i] = self.loss(yOneHot,predictionProb,batchSize)\n",
    "\n",
    "            #Calculer le gradient\n",
    "            grad = self.gradient(sampleTrain,yOneHot,predictionProb,lam,batchSize)\n",
    "\n",
    "            #Ameliorer W et B\n",
    "            self.weights = self.weights + alpha*grad[:-1,:]\n",
    "            self.bias = self.bias + alpha*grad[-1,:]\n",
    "            \n",
    "        return grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m logReg \u001b[39m=\u001b[39m LogisiticRegression(trainData,trainLabels)\n\u001b[1;32m----> 2\u001b[0m x \u001b[39m=\u001b[39m logReg\u001b[39m.\u001b[39;49mtrain(\u001b[39m1\u001b[39;49m,\u001b[39m12\u001b[39;49m,\u001b[39m0.8\u001b[39;49m,\u001b[39m0.1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [114], line 83\u001b[0m, in \u001b[0;36mLogisiticRegression.train\u001b[1;34m(self, iter, batchSize, lam, alpha)\u001b[0m\n\u001b[0;32m     80\u001b[0m loss[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(yOneHot,predictionProb,batchSize)\n\u001b[0;32m     82\u001b[0m \u001b[39m#Calculer le gradient\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(sampleTrain,yOneHot,predictionProb,lam,batchSize)\n\u001b[0;32m     85\u001b[0m \u001b[39m#Ameliorer W et B\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m+\u001b[39m alpha\u001b[39m*\u001b[39mgrad[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n",
      "Cell \u001b[1;32mIn [114], line 65\u001b[0m, in \u001b[0;36mLogisiticRegression.gradient\u001b[1;34m(self, X, Y, Yhat, lam, bathSize)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m,X,Y,Yhat,lam,bathSize):\n\u001b[0;32m     63\u001b[0m     \u001b[39m#retourner le gradient avec une regularisation L2 \u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[39m#Ajouter le biais au X et W pour avoir son gradient\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mtranspose()\u001b[39m.\u001b[39;49mconcat(pd\u001b[39m.\u001b[39mSeries(\u001b[39m1\u001b[39m,index\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mindex),ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m     W \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mconcat(pd\u001b[39m.\u001b[39mSeries(\u001b[39m1\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabelSize)),ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m (X\u001b[39m.\u001b[39mdot((Y\u001b[39m-\u001b[39mYhat\u001b[39m.\u001b[39mtranspose())\u001b[39m.\u001b[39mtranspose()\u001b[39m.\u001b[39mto_numpy()))\u001b[39m/\u001b[39mbathSize \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mlam\u001b[39m*\u001b[39mW\u001b[39m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32mc:\\Users\\vgari\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "logReg = LogisiticRegression(trainData,trainLabels)\n",
    "x = logReg.train(1,12,0.8,0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05d8f42d34e97b63e1918e352e5e1ee86173089fb3b8c3e567ea1c06d83cd6aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
